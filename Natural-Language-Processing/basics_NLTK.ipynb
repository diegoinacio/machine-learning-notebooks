{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basics [NLTK]\n",
    "---\n",
    "- Author: Diego In√°cio\n",
    "- GitHub: [github.com/diegoinacio](https://github.com/diegoinacio)\n",
    "- Notebook: [basics_NLTK.ipynb](https://github.com/diegoinacio/machine-learning-notebooks/blob/master/Natural-Language-Processing/basics_NLTK.ipynb)\n",
    "---\n",
    "Basics aspects of [Natural Language Toolkit](https://www.nltk.org/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Installation and download data files\n",
    "---\n",
    "\n",
    "[Installation](http://numba.pydata.org/numba-doc/latest/user/installing.html) command for *anaconda* and *pip*:\n",
    "\n",
    "```\n",
    "$ conda install --channel anaconda nltk\n",
    "```\n",
    "\n",
    "or\n",
    "\n",
    "```\n",
    "$ pip install nltk\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To install all the data requirement for NLTK, first define the output directory and download it by running:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = 'D:/GitHub/machine-learning-notebooks/Natural-Language-Processing/nltk_data'\n",
    "nltk.data.path.append(PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download(download_dir=PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Tokenize\n",
    "---\n",
    "*Tokenization* is the process of breaking up a text into pieces of text called *Token*. Tokenization can happen at several different levels, like: paragraphs, sentences, words, syllables, or phonemes.\n",
    "\n",
    "Given the example text:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'Hello everyone, how are you all? This is an example of text, which will be tokenized in several ways. Thank you!'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1. Tokenize sentences\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello everyone, how are you all?\n",
      "This is an example of text, which will be tokenized in several ways.\n",
      "Thank you!\n"
     ]
    }
   ],
   "source": [
    "sentences = sent_tokenize(text)\n",
    "for sentence in sentences:\n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2. Tokenize words\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello\n",
      "everyone\n",
      ",\n",
      "how\n",
      "are\n",
      "you\n",
      "all\n",
      "?\n",
      "This\n",
      "is\n",
      "an\n",
      "example\n",
      "of\n",
      "text\n",
      ",\n",
      "which\n",
      "will\n",
      "be\n",
      "tokenized\n",
      "in\n",
      "several\n",
      "ways\n",
      ".\n",
      "Thank\n",
      "you\n",
      "!\n"
     ]
    }
   ],
   "source": [
    "words = word_tokenize(text)\n",
    "for word in words:\n",
    "    print(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Penn Part of Speech Tags\n",
    "---\n",
    "Alphabetical list of part-of-speech tags used in the Penn Treebank Project.\n",
    "\n",
    "number | tag  | description\n",
    "-------|------|-------------\n",
    "1      | CC\t  | Coordinating conjunction\n",
    "2      | CD\t  | Cardinal number\n",
    "3      | DT\t  | Determiner\n",
    "4      | EX\t  | Existential there\n",
    "5      | FW\t  | Foreign word\n",
    "6      | IN\t  | Preposition or subordinating conjunction\n",
    "7      | JJ\t  | Adjective\n",
    "8      | JJR  | Adjective, comparative\n",
    "9      | JJS  | Adjective, superlative\n",
    "10     | LS\t  | List item marker\n",
    "11     | MD\t  | Modal\n",
    "12     | NN\t  | Noun, singular or mass\n",
    "13     | NNS  | Noun, plural\n",
    "14     | NNP  | Proper noun, singular\n",
    "15     | NNPS | Proper noun, plural\n",
    "16     | PDT  | Predeterminer\n",
    "17     | POS  | Possessive ending\n",
    "18     | PRP  | Personal pronoun\n",
    "19     | PRP\\$| Possessive pronoun\n",
    "20     | RB\t  | Adverb\n",
    "21     | RBR  | Adverb, comparative\n",
    "22     | RBS  | Adverb, superlative\n",
    "23     | RP\t  | Particle\n",
    "24     | SYM  | Symbol\n",
    "25     | TO\t  | to\n",
    "26     | UH\t  | Interjection\n",
    "27     | VB\t  | Verb, base form\n",
    "28     | VBD  | Verb, past tense\n",
    "29     | VBG  | Verb, gerund or present participle\n",
    "30     | VBN  | Verb, past participle\n",
    "31     | VBP  | Verb, non-3rd person singular present\n",
    "32     | VBZ  | Verb, 3rd person singular present\n",
    "33     | WDT  | Wh-determiner\n",
    "34     | WP\t  | Wh-pronoun\n",
    "35     | WP\\$ | Possessive wh-pronoun\n",
    "36     | WRB  | Wh-adverb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Hello', 'NNP'), ('everyone', 'NN'), (',', ','), ('how', 'WRB'), ('are', 'VBP'), ('you', 'PRP'), ('all', 'DT'), ('?', '.'), ('This', 'DT'), ('is', 'VBZ'), ('an', 'DT'), ('example', 'NN'), ('of', 'IN'), ('text', 'NN'), (',', ','), ('which', 'WDT'), ('will', 'MD'), ('be', 'VB'), ('tokenized', 'VBN'), ('in', 'IN'), ('several', 'JJ'), ('ways', 'NNS'), ('.', '.'), ('Thank', 'NNP'), ('you', 'PRP'), ('!', '.')]\n"
     ]
    }
   ],
   "source": [
    "tags = nltk.pos_tag(words)\n",
    "print(tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Chunking\n",
    "---\n",
    "[Chunking](http://www.nltk.org/howto/chunk.html) uses a special syntax for *regular expressions* rules that delimit the [chunks](https://www.nltk.org/api/nltk.chunk.html).\n",
    "\n",
    "For the following example, lets find any *noun* or *proper noun* (NN, NNS, NNP or NNPS) followed by a punctuation mark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "rule = r'Chunk: {<NN[SP]*.?>+<.>}'\n",
    "parser = nltk.RegexpParser(rule)\n",
    "chunk = parser.parse(tags)\n",
    "\n",
    "chunk.draw()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![chunk tree](chunk_tree_temp.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Stemming\n",
    "---\n",
    "[Stemming](https://www.nltk.org/howto/stem.html) removes all morphological affixes from words and leaves only the word stem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenization -> token\n",
      "running -> run\n",
      "pythonic -> python\n",
      "understandable -> understand\n",
      "avoidable -> avoid\n",
      "memorable -> memor\n"
     ]
    }
   ],
   "source": [
    "words = ['tokenization', 'running', 'pythonic', 'understandable', 'avoidable', 'memorable']\n",
    "PS = PorterStemmer()\n",
    "for word in words:\n",
    "        print(f'{word} -> {PS.stem(word)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Lemmatization\n",
    "---\n",
    "Lemmatization is the process of converting a word to its meaningful base form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "children -> child\n",
      "feet -> foot\n",
      "wolves -> wolf\n",
      "indices -> index\n",
      "leaves -> leaf\n",
      "mice -> mouse\n",
      "phenomena -> phenomenon\n"
     ]
    }
   ],
   "source": [
    "words = ['children', 'feet', 'wolves', 'indices', 'leaves', 'mice', 'phenomena']\n",
    "WL = WordNetLemmatizer()\n",
    "for word in words:\n",
    "        print(f'{word} -> {WL.lemmatize(word)}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
