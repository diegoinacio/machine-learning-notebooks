{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basics [NLTK]\n",
    "---\n",
    "- Author: Diego In√°cio\n",
    "- GitHub: [github.com/diegoinacio](https://github.com/diegoinacio)\n",
    "- Notebook: [basics_NLTK.ipynb](https://github.com/diegoinacio/machine-learning-notebooks/blob/master/Natural-Language-Processing/basics_NLTK.ipynb)\n",
    "---\n",
    "Basics aspects of [Natural Language Toolkit](https://www.nltk.org/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation and download data files\n",
    "---\n",
    "\n",
    "[Installation](http://numba.pydata.org/numba-doc/latest/user/installing.html) command for *anaconda* and *pip*:\n",
    "\n",
    "```\n",
    "$ conda install --channel anaconda nltk\n",
    "```\n",
    "\n",
    "or\n",
    "\n",
    "```\n",
    "$ pip install nltk\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To install all the data requirement for NLTK, first define the output directory and download it by running:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = 'D:/GitHub/machine-learning-notebooks/Natural-Language-Processing/nltk_data'\n",
    "nltk.data.path.append(PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download(download_dir=PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize\n",
    "---\n",
    "*Tokenization* is the process of breaking up a text into pieces of text called *Token*. Tokenization can happen at several different levels, like: paragraphs, sentences, words, syllables, or phonemes.\n",
    "\n",
    "Given the example text:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'Hello everyone, how are you all? This is an example of text, which will be tokenized in several ways. Thank you!'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize sentences\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = sent_tokenize(text)\n",
    "for sentence in sentences:\n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize words\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = word_tokenize(text)\n",
    "for word in words:\n",
    "    print(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Penn Part of Speech Tags\n",
    "---\n",
    "Alphabetical list of part-of-speech tags used in the Penn Treebank Project.\n",
    "\n",
    "number | tag  | description\n",
    "-------|------|-------------\n",
    "1      | CC\t  | Coordinating conjunction\n",
    "2      | CD\t  | Cardinal number\n",
    "3      | DT\t  | Determiner\n",
    "4      | EX\t  | Existential there\n",
    "5      | FW\t  | Foreign word\n",
    "6      | IN\t  | Preposition or subordinating conjunction\n",
    "7      | JJ\t  | Adjective\n",
    "8      | JJR  | Adjective, comparative\n",
    "9      | JJS  | Adjective, superlative\n",
    "10     | LS\t  | List item marker\n",
    "11     | MD\t  | Modal\n",
    "12     | NN\t  | Noun, singular or mass\n",
    "13     | NNS  | Noun, plural\n",
    "14     | NNP  | Proper noun, singular\n",
    "15     | NNPS | Proper noun, plural\n",
    "16     | PDT  | Predeterminer\n",
    "17     | POS  | Possessive ending\n",
    "18     | PRP  | Personal pronoun\n",
    "19     | PRP\\$| Possessive pronoun\n",
    "20     | RB\t  | Adverb\n",
    "21     | RBR  | Adverb, comparative\n",
    "22     | RBS  | Adverb, superlative\n",
    "23     | RP\t  | Particle\n",
    "24     | SYM  | Symbol\n",
    "25     | TO\t  | to\n",
    "26     | UH\t  | Interjection\n",
    "27     | VB\t  | Verb, base form\n",
    "28     | VBD  | Verb, past tense\n",
    "29     | VBG  | Verb, gerund or present participle\n",
    "30     | VBN  | Verb, past participle\n",
    "31     | VBP  | Verb, non-3rd person singular present\n",
    "32     | VBZ  | Verb, 3rd person singular present\n",
    "33     | WDT  | Wh-determiner\n",
    "34     | WP\t  | Wh-pronoun\n",
    "35     | WP\\$ | Possessive wh-pronoun\n",
    "36     | WRB  | Wh-adverb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tags = nltk.pos_tag(words)\n",
    "print(tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chunking\n",
    "---\n",
    "[Chunking](http://www.nltk.org/howto/chunk.html) uses a special syntax for *regular expressions* rules that delimit the [chunks](https://www.nltk.org/api/nltk.chunk.html).\n",
    "\n",
    "For the following example, lets find any *noun* or *proper noun* (NN, NNS, NNP or NNPS) followed by a punctuation mark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rule = r'Chunk: {<NN[SP]*.?>+<.>}'\n",
    "parser = nltk.RegexpParser(rule)\n",
    "chunk = parser.parse(tags)\n",
    "\n",
    "chunk.draw()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![chunk tree](chunk_tree_temp.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming\n",
    "---\n",
    "[Stemming](https://www.nltk.org/howto/stem.html) removes all morphological affixes from words and leaves only the word stem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = ['tokenization', 'running', 'pythonic', 'understandable', 'avoidable', 'memorable']\n",
    "PS = PorterStemmer()\n",
    "for word in words:\n",
    "        print(f'{word} -> {PS.stem(word)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmatization\n",
    "---\n",
    "Lemmatization is the process of converting a word to its meaningful base form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = ['children', 'feet', 'wolves', 'indices', 'leaves', 'mice', 'phenomena']\n",
    "WL = WordNetLemmatizer()\n",
    "for word in words:\n",
    "        print(f'{word} -> {WL.lemmatize(word)}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
