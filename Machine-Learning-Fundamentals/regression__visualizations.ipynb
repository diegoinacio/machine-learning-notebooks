{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression - Visualizations\n",
    "---\n",
    "- Author: Diego Inácio\n",
    "- GitHub: [github.com/diegoinacio](https://github.com/diegoinacio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as manim\n",
    "from matplotlib import cm\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import numpy as np\n",
    "import imageio as iio\n",
    "\n",
    "from regression__utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = (16, 8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Linear Regression\n",
    "---\n",
    "### 1.1. Simple\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class linearRegression_simple(object):\n",
    "    def __init__(self):\n",
    "        self._m = 0\n",
    "        self._b = 0\n",
    "    @property\n",
    "    def m(self):\n",
    "        return self._m\n",
    "    @property\n",
    "    def b(self):\n",
    "        return self._b\n",
    "    def fit(self, X, y):\n",
    "        X = np.array(X)\n",
    "        y = np.array(y)\n",
    "        X_ = X.mean()\n",
    "        y_ = y.mean()\n",
    "        num = ((X - X_)*(y - y_)).sum()\n",
    "        den = ((X - X_)**2).sum()\n",
    "        self._m = num/den\n",
    "        self._b = y_ - self._m*X_\n",
    "    def pred(self, x):\n",
    "        x = np.array(x)\n",
    "        return self._m*x + self._b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lrs = linearRegression_simple()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Synthetic data 1\n",
    "x, yA, yB, yC, yD = synthData1()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "def cellPlot(ax, x, y, lrs):\n",
    "    ax.scatter(x, y, 32)\n",
    "    ax.set_title('Correlation (ρ = {0:.2f})'.\n",
    "                 format(correlation(x, y)))\n",
    "    ax.grid(color = '0.9', linestyle = ':')\n",
    "    ax.axis([-0.1, 1.1, -0.5, 1.5])\n",
    "\n",
    "fig, [[axA, axB], [axC, axD]] = plt.subplots(2, 2)\n",
    "\n",
    "cellPlot(axA, x, yA, lrs)\n",
    "cellPlot(axB, x, yB, lrs)\n",
    "cellPlot(axC, x, yC, lrs)\n",
    "cellPlot(axD, x, yD, lrs)\n",
    "\n",
    "fig.savefig('output/regression_linear_correlation.png', tight_layout=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "def cellPlot(ax, x, y, lrs):\n",
    "    ax.scatter(x, y, 32, label='data')\n",
    "    ax.plot([x, x], [y, lrs.pred(x)], ':')\n",
    "    ax.plot(0, 0, ':', alpha=0.5, label='error')\n",
    "    ax.plot([-5, 5], lrs.pred([-5, 5]), color='red', label='regression')\n",
    "    ax.set_title('ρ = {0:.2f}, m = {1:.2f}, b = {2:.2f}'.\n",
    "                 format(correlation(x, y), lrs.m, lrs.b))\n",
    "    ax.grid(color='0.9', linestyle=':')\n",
    "    ax.axis([-0.1, 1.1, -0.5, 1.5])\n",
    "    ax.legend()\n",
    "\n",
    "fig, [[axA, axB], [axC, axD]] = plt.subplots(2, 2)\n",
    "\n",
    "lrs.fit(x, yA)\n",
    "cellPlot(axA, x, yA, lrs)\n",
    "\n",
    "lrs.fit(x, yB)\n",
    "cellPlot(axB, x, yB, lrs)\n",
    "\n",
    "lrs.fit(x, yC)\n",
    "cellPlot(axC, x, yC, lrs)\n",
    "\n",
    "lrs.fit(x, yD)\n",
    "cellPlot(axD, x, yD, lrs)\n",
    "\n",
    "fig.savefig('output/regression_linear_pred.png', tight_layout=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "def cellPlot(ax, x, y, lrs):\n",
    "    erro = y - lrs.pred(x)\n",
    "    MSE = (erro**2).sum()/erro.size\n",
    "    ax.plot([x, x], [erro*0, erro])\n",
    "    ax.set_title('MSE = {0:.2f}'.format(MSE))\n",
    "    ax.grid(color = '0.9', linestyle = ':')\n",
    "    ax.axis([-0.1, 1.1, -0.75, 0.75])\n",
    "\n",
    "fig, [[axA, axB], [axC, axD]] = plt.subplots(2, 2)\n",
    "\n",
    "lrs.fit(x, yA)\n",
    "cellPlot(axA, x, yA, lrs)\n",
    "\n",
    "lrs.fit(x, yB)\n",
    "cellPlot(axB, x, yB, lrs)\n",
    "\n",
    "lrs.fit(x, yC)\n",
    "cellPlot(axC, x, yC, lrs)\n",
    "\n",
    "lrs.fit(x, yD)\n",
    "cellPlot(axD, x, yD, lrs)\n",
    "\n",
    "fig.savefig('output/regression_linear_residual.png', tight_layout=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Multiple\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class linearRegression_multiple(object):\n",
    "    def __init__(self):\n",
    "        self._m = 0\n",
    "        self._b = 0\n",
    "    @property\n",
    "    def m(self):\n",
    "        return self._m\n",
    "    @property\n",
    "    def b(self):\n",
    "        return self._b[0]\n",
    "    def fit(self, X, y):\n",
    "        X = np.array(X).T\n",
    "        y = np.array(y).reshape(-1, 1)\n",
    "        X_ = X.mean(axis = 0)\n",
    "        y_ = y.mean(axis = 0)\n",
    "        num = ((X - X_)*(y - y_)).sum(axis = 0)\n",
    "        den = ((X - X_)**2).sum(axis = 0)\n",
    "        self._m = num/den\n",
    "        self._b = y_ - (self._m*X_).sum()\n",
    "    def pred(self, x):\n",
    "        x = np.array(x).T\n",
    "        return (self._m*x).sum(axis = 1) + self._b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lrm = linearRegression_multiple()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Synthetic data 2\n",
    "M = 10\n",
    "s, t, x1, x2, y = synthData2(M)\n",
    "lrm.fit([x1, x2], y)\n",
    "####################\n",
    "px, py = s, t\n",
    "p = lrm.pred([x1, x2])\n",
    "up = y >= p\n",
    "down = y < p\n",
    "pz = p.reshape(M, M)\n",
    "####################\n",
    "cmapUp = cm.get_cmap('spring')\n",
    "cmapDown = cm.get_cmap('winter')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "def cellPlot(ax, m, alt, azi):\n",
    "    ax.view_init(alt, azi)\n",
    "    ax.scatter3D(x1[up], x2[up], y[up], zorder = 1)\n",
    "    for i, j, k, l in zip(x1[up], x2[up], y[up], p[up]):\n",
    "        ax.plot([i, i], [j, j], [k, l], ':',\n",
    "                zorder = 2, alpha = 0.5,\n",
    "                color = cmapUp((i**2 + j**2)**0.5))\n",
    "    for i, j, k, l in zip(x1[down], x2[down], y[down], p[down]):\n",
    "        ax.plot([i, i], [j, j], [k, l], ':',\n",
    "                zorder=5, alpha=0.5,\n",
    "                color=cmapDown((i**2 + j**2)**0.5))\n",
    "    ax.scatter3D(x1[down], x2[down], y[down], zorder=4)\n",
    "    ax.plot_surface(px*m, py*m, pz*m,\n",
    "                    rstride=M,\n",
    "                    cstride=M,\n",
    "                    color='red',\n",
    "                    alpha=0.5,\n",
    "                    zorder=3)\n",
    "\n",
    "fig, (axA, axB) = plt.subplots(1, 2, subplot_kw={'projection': '3d'})\n",
    "fig.suptitle('Linear Regresion Multiple\\nm1 = {0:.2f}, m2 = {1:.2f}, b = {2:.2f}'.format(*lrm.m, lrm.b))\n",
    "\n",
    "cellPlot(axA, 1.75, 20, -250)\n",
    "cellPlot(axB, 1.75, 5, -50)\n",
    "\n",
    "fig.savefig('output/regression_linear_multiple_pred.png', tight_layout=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "def cellPlot(ax, x1, x2, y, m, alt, azi):\n",
    "    ax.view_init(alt, azi)\n",
    "    error = y - p\n",
    "    MSE = (error**2).sum()/error.size\n",
    "    for i, j, e in zip(x1[up], x2[up], error[up]):\n",
    "        ax.plot([i, i], [j, j], [e*0, e], zorder=1, color=cmapUp((i**2 + j**2)**0.5))\n",
    "    for i, j, e in zip(x1[down], x2[down], error[down]):\n",
    "        ax.plot([i, i], [j, j], [e*0, e], ':', zorder=2, color = cmapDown((i**2 + j**2)**0.5))\n",
    "    fig.suptitle('Multiple Linear Regression\\nMSE = {0:.2f}'.format(MSE))\n",
    "\n",
    "fig, (axA, axB) = plt.subplots(1, 2, subplot_kw={'projection': '3d'})\n",
    "\n",
    "cellPlot(axA, x1, x2, y, 1.75, 20, -150)\n",
    "cellPlot(axB, x1, x2, y, 1.75, 5, -85)\n",
    "\n",
    "fig.savefig('output/regression_linear_multipla_residual.png', tight_layout=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3. Gradient Descent\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class linearRegression_GD(object):\n",
    "    def __init__(self,\n",
    "                 mo = 0,\n",
    "                 bo = 0,\n",
    "                 rate = 0.001):\n",
    "        self._m = mo\n",
    "        self._b = bo\n",
    "        self.rate = rate\n",
    "    @property\n",
    "    def m(self):\n",
    "        return self._m\n",
    "    @property\n",
    "    def b(self):\n",
    "        return self._b\n",
    "    def fit_step(self, X, y):\n",
    "        X = np.array(X)\n",
    "        y = np.array(y)\n",
    "        n = X.size\n",
    "        dm = (2/n)*np.sum(-x*(y - (self.m*x + self.b)))\n",
    "        db = (2/n)*np.sum(-(y - (self.m*x + self.b)))\n",
    "        self._m -= dm*self.rate\n",
    "        self._b -= db*self.rate\n",
    "    def pred(self, x):\n",
    "        x = np.array(x)\n",
    "        return self._m*x + self._b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "def cellPlot(ax, title='',\n",
    "             sx='', sy='',\n",
    "             xlim=[-0.1, 1.1],\n",
    "             ylim=[-0.5, 1.5],\n",
    "             leg=True):\n",
    "    ax.set_title(title)\n",
    "    ax.grid(color='0.9', linestyle=':')\n",
    "    ax.axis([*xlim, *ylim])\n",
    "    ax.set_xlabel(sx)\n",
    "    ax.set_ylabel(sy)\n",
    "    if leg:\n",
    "        ax.legend()\n",
    "\n",
    "fig, [[axA, axB], [axC, axD]] = plt.subplots(2, 2)\n",
    "\n",
    "lrs = linearRegression_simple()\n",
    "lrgd = linearRegression_GD(rate=0.01)\n",
    "\n",
    "# Synthetic data 3\n",
    "x, x_, y = synthData3()\n",
    "\n",
    "lrs.fit(x, y)\n",
    "\n",
    "B, M, I, E = [], [], [], []\n",
    "iterations = 3072\n",
    "nframes = 64\n",
    "counter = 0\n",
    "def animation(frame):\n",
    "    global B, M, I, E, counter\n",
    "    for i_ in range(48):\n",
    "        B += [lrgd.b]\n",
    "        M += [lrgd.m]\n",
    "        I += [counter]\n",
    "        error = y - lrgd.pred(x)\n",
    "        e = np.sum(error**2)/x.size\n",
    "        E += [e]\n",
    "        lrgd.fit_step(x, y)\n",
    "        counter += 1\n",
    "    i = counter\n",
    "\n",
    "    axA.cla(); axB.cla(); axC.cla(); axD.cla();\n",
    "    axA.scatter(x, y, 32, label='data')\n",
    "    axA.plot(x_, lrs.pred(x_), '--', color='red',\n",
    "             label='linear regression')\n",
    "    axA.plot(x_, lrgd.pred(x_), color='green',\n",
    "             label='gradient descent')\n",
    "    cellPlot(axA)\n",
    "    axB.plot([x, x], [error*0, error])\n",
    "    cellPlot(axB, ylim=[-1, 1], leg = False)\n",
    "    axC.plot(B, M)\n",
    "    cellPlot(axC, 'b = {0:.3f}, m = {1:.3f}'.format(lrgd.b, lrgd.m),\n",
    "             xlim = [-0.5, 0.5],\n",
    "             sx = 'linear coeficient (b)',\n",
    "             sy = 'angular coeficient (m)',\n",
    "             leg = False)\n",
    "    axD.plot(I, E)\n",
    "    axD.set_xscale('symlog', nonposx='clip')\n",
    "    cellPlot(axD, r'iteration = {0:04d}, $\\epsilon$ = {1:.4f}'.format(i, e),\n",
    "             xlim = [0, iterations],\n",
    "             ylim = [-0.1, 0.6],\n",
    "             sx = 'iterations (log)',\n",
    "             sy = 'error',\n",
    "             leg = False)\n",
    "    return fig.canvas.draw()\n",
    "\n",
    "anim = manim.FuncAnimation(fig, animation, frames=nframes, interval=100)\n",
    "    \n",
    "anim.save('output/regression_linear_gradDesc.gif', writer=\"imagemagick\", extra_args=\"convert\")\n",
    "plt.close()\n",
    "\n",
    "# Solve repetition problem\n",
    "! magick convert output/regression_linear_gradDesc.gif -loop 0 output/regression_linear_gradDesc.gif\n",
    "! echo GIF exported and reconverted. Disregard any message above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4. Non-linear Analysis\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Synthetic data 4\n",
    "# Anscombe's quartet\n",
    "x1, y1, x2, y2, x3, y3, x4, y4 = synthData4()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "def cellPlot(ax, x, y, lrs):\n",
    "    ax.scatter(x, y, 32, label='data')\n",
    "    ax.plot([x, x], [y, lrs.pred(x)], ':')\n",
    "    ax.plot(0, 0, ':', alpha=0.5, label='error')\n",
    "    ax.plot([-50, 50], lrs.pred([-50, 50]), color='red', label='regression')\n",
    "    ax.set_title('ρ = {0:.2f}, m = {1:.2f}, b = {2:.2f}'.\n",
    "                 format(correlation(x, y), lrs.m, lrs.b))\n",
    "    ax.grid(color='0.9', linestyle=':')\n",
    "    ax.axis([-0, 20, 0, 15])\n",
    "    ax.legend()\n",
    "\n",
    "fig, [[axA, axB], [axC, axD]] = plt.subplots(2, 2)\n",
    "\n",
    "lrs.fit(x1, y1)\n",
    "cellPlot(axA, x1, y1, lrs)\n",
    "\n",
    "lrs.fit(x2, y2)\n",
    "cellPlot(axB, x2, y2, lrs)\n",
    "\n",
    "lrs.fit(x3, y3)\n",
    "cellPlot(axC, x3, y3, lrs)\n",
    "\n",
    "lrs.fit(x4, y4)\n",
    "cellPlot(axD, x4, y4, lrs)\n",
    "\n",
    "fig.savefig('output/regression_linear_anscombe_pred.png', tight_layout=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "def cellPlot(ax, x, y, lrs):\n",
    "    error = y - lrs.pred(x)\n",
    "    MSE = (error**2).sum()/error.size\n",
    "    ax.plot([x, x], [error*0, error])\n",
    "    ax.set_title('MSE = {0:.2f}'.format(MSE))\n",
    "    ax.grid(color='0.9', linestyle=':')\n",
    "    ax.axis([-0, 20, -5, 5])\n",
    "\n",
    "fig, [[axA, axB], [axC, axD]] = plt.subplots(2, 2)\n",
    "\n",
    "lrs.fit(x1, y1)\n",
    "cellPlot(axA, x1, y1, lrs)\n",
    "\n",
    "lrs.fit(x2, y2)\n",
    "cellPlot(axB, x2, y2, lrs)\n",
    "\n",
    "lrs.fit(x3, y3)\n",
    "cellPlot(axC, x3, y3, lrs)\n",
    "\n",
    "lrs.fit(x4, y4)\n",
    "cellPlot(axD, x4, y4, lrs)\n",
    "\n",
    "fig.savefig('output/regression_linear_anscombe_residual.png', tight_layout=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Logistic Regression\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def arraycast(f):\n",
    "    '''\n",
    "    Decorator for vectors and matrices cast\n",
    "    '''\n",
    "    def wrap(self, *X, y=[]):\n",
    "        X = np.array(X)\n",
    "        X = np.insert(X.T, 0, 1, 1)\n",
    "        if list(y):\n",
    "            y = np.array(y)[np.newaxis]\n",
    "            return f(self, X, y)\n",
    "        return f(self, X)\n",
    "    return wrap\n",
    "\n",
    "class logisticRegression(object):\n",
    "    def __init__(self, rate=0.001, iters=1024):\n",
    "        self._rate = rate\n",
    "        self._iters = iters\n",
    "        self._theta = None\n",
    "    @property\n",
    "    def theta(self):\n",
    "        return self._theta\n",
    "    def _sigmoid(self, Z):\n",
    "        return 1/(1 + np.exp(-Z))\n",
    "    def _dsigmoid(self, Z):\n",
    "        return self._sigmoid(Z)*(1 - self._sigmoid(Z))\n",
    "    @arraycast\n",
    "    def fit(self, X, y=[]):\n",
    "        self._theta = np.ones((1, X.shape[-1]))\n",
    "        for i in range(self._iters):\n",
    "            thetaTx = np.dot(X, self._theta.T)\n",
    "            h = self._sigmoid(thetaTx)\n",
    "            delta = h - y.T\n",
    "            grad = np.dot(X.T, delta).T\n",
    "            self._theta -= grad*self._rate\n",
    "    @arraycast\n",
    "    def pred(self, x):\n",
    "        return self._sigmoid(np.dot(x, self._theta.T)) > 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1)\n",
    "\n",
    "# Synthetic data 5\n",
    "x1, x2, y = synthData5()\n",
    "\n",
    "ax.scatter(x1, x2, c=y)\n",
    "\n",
    "ax.set_xlim([0, 3])\n",
    "ax.set_ylim([-1, 1.25])\n",
    "\n",
    "fig.savefig('output/regression_logistic_data.png', tight_layout=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "def cellPlot(ax, titulo='',\n",
    "             sx='', sy='',\n",
    "             xlim=[0, 3],\n",
    "             ylim=[-1, 1.25],\n",
    "             leg=False):\n",
    "    ax.set_title(titulo)\n",
    "    ax.grid(color='0.9', linestyle=':')\n",
    "    ax.axis([*xlim, *ylim])\n",
    "    ax.set_xlabel(sx)\n",
    "    ax.set_ylabel(sy)\n",
    "    if leg:\n",
    "        ax.legend()\n",
    "\n",
    "fig, [axA, axB] = plt.subplots(2, 1)\n",
    "\n",
    "# Synthetic data 5\n",
    "x1, x2, y = synthData5()\n",
    "\n",
    "x_ = np.linspace(-10, 10)\n",
    "\n",
    "iteracoes = 512\n",
    "nframes = 64\n",
    "frames = 1 + np.arange(nframes + 1)*iteracoes//nframes\n",
    "def animation(frame):\n",
    "    axA.cla(); axB.cla();\n",
    "    \n",
    "    rlogb = logisticRegression(rate=0.001, iters=frames[frame])\n",
    "    rlogb.fit(x1, x2, y=y)\n",
    "    w0, w1, w2 = rlogb.theta[0]\n",
    "    f = lambda x: (- w0 - w1*x)/w2\n",
    "    \n",
    "    axA.scatter(x1, x2, c=y)\n",
    "    axA.plot(x_, f(x_))\n",
    "    cellPlot(axA)\n",
    "    \n",
    "    diff = rlogb.pred(x1, x2).T[0] == y\n",
    "    axB.scatter(x1[~diff], x2[~diff], c='#DD0033')\n",
    "    axB.scatter(x1[diff], x2[diff], c='#33AA00')\n",
    "    axB.plot(x_, f(x_))\n",
    "    cellPlot(axB)\n",
    "    \n",
    "    return fig.canvas.draw()\n",
    "\n",
    "anim = manim.FuncAnimation(fig, animation, frames=nframes, interval=100)\n",
    "    \n",
    "anim.save('output/regression_logistic_gradDesc.gif', writer=\"imagemagick\", extra_args=\"convert\")\n",
    "plt.close()\n",
    "\n",
    "# Solve repetition problem\n",
    "! magick convert output/regression_logistic_gradDesc.gif -loop 0 output/regression_logistic_gradDesc.gif\n",
    "! echo GIF exported and reconverted. Disregard any message above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rlogb = logisticRegression(rate=0.001, iters=512)\n",
    "rlogb.fit(x1, x2, y=y)\n",
    "\n",
    "w0, w1, w2 = rlogb.theta[0]\n",
    "f = lambda x: (- w0 - w1*x)/w2\n",
    "\n",
    "np.random.seed(sum([ord(c) for c in 'Regression']))\n",
    "X1, X2 = np.random.uniform(-2, 5, 128), np.random.uniform(-1, 1, 128)\n",
    "\n",
    "Y = rlogb.pred(X1, X2)\n",
    "plt.scatter(X1, X2, c = Y[:,0])\n",
    "plt.plot(x_, f(x_), '--')\n",
    "\n",
    "plt.xlim([-2.1, 5.1])\n",
    "plt.ylim([-1.1, 1.1])\n",
    "\n",
    "plt.savefig('output/regression_logistic_pred.png', tight_layout=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Polynomial Regression\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Synthetic data 6\n",
    "x, y = synthData6()\n",
    "\n",
    "lrs = linearRegression_simple()\n",
    "lrs.fit(x, y)\n",
    "\n",
    "plt.scatter(x, y, label='Sample')\n",
    "plt.plot(x, lrs.pred(x), label='Linear Regression', c='red')\n",
    "plt.legend()\n",
    "\n",
    "plt.savefig('output/regression_polynomial_linear.png', tight_layout=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def arraycast(f):\n",
    "    '''\n",
    "    Decorator for vectors and matrices casting\n",
    "    '''\n",
    "    def wrap(self, X, y=[]):\n",
    "        X = np.array(X)\n",
    "        if list(y):\n",
    "            y = np.array(y)\n",
    "            return f(self, X, y)\n",
    "        return f(self, X)\n",
    "    return wrap\n",
    "\n",
    "class polynomialRegression(object):\n",
    "    def __init__(self, degree=1):\n",
    "        self._degree = degree\n",
    "        self._beta = None\n",
    "    @property\n",
    "    def beta(self):\n",
    "        return self._beta\n",
    "    @arraycast\n",
    "    def fit(self, X, y=[]):\n",
    "        V = np.stack([X**i for i in range(self._degree + 1)], axis=0).T\n",
    "        VTV = np.dot(V.T, V)\n",
    "        VTV_i = np.linalg.inv(VTV)\n",
    "        Vi = np.dot(VTV_i, V.T)\n",
    "        self._beta = np.dot(Vi, y)\n",
    "    @arraycast\n",
    "    def pred(self, x):\n",
    "        V = np.stack([x**i for i in range(self._degree + 1)], axis=0).T\n",
    "        return np.dot(V, self._beta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "polreg = polynomialRegression(3)\n",
    "polreg.fit(x, y=y)\n",
    "\n",
    "X = np.linspace(-3, 3, 101)\n",
    "plt.scatter(x, y)\n",
    "plt.plot(X, polreg.pred(X), c='#88DD00', label='Polynomial Regression')\n",
    "plt.plot(X, X**3 - 3*X**2 + X + 1, ':', c='black', label='Ground truth f(x)')\n",
    "plt.legend()\n",
    "\n",
    "plt.savefig('output/regression_polynomial_pred.png')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
