{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network - Perceptron\n",
    "---\n",
    "- Author: Diego InÃ¡cio\n",
    "- GitHub: [github.com/diegoinacio](https://github.com/diegoinacio)\n",
    "- Notebook: [NN_perceptron.ipynb](https://github.com/diegoinacio/machine-learning-notebooks/blob/master/Machine-Learning-Fundamentals/NN_perceptron.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from NN__utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Synthetic data 1 and 2\n",
    "x1, y1 = synthData1()\n",
    "x2, y2 = synthData2()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![nn perceptron synthdata](output/NN_perceptron_synthData.png \"Neural Network Perceptron Synthetic Data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Perceptron\n",
    "---\n",
    "The *Perceptron* algorithm is very similar to the *Logistic Regression* one.\n",
    "### 1.1. Foward propagation\n",
    "---\n",
    "$$ \\large\n",
    "Z=w^TX + b\n",
    "$$\n",
    "The activation function (*Sigmoid*) is:\n",
    "$$ \\large\n",
    "\\hat{y}=A=\\frac{1}{1 + e^{-Z}}\n",
    "$$\n",
    "The *cost* function is:\n",
    "$$ \\large\n",
    "J = -\\frac{1}{m}\\sum_{i=1}^{m}y^{(i)}\\log(a^{(i)})+(1-y^{(i)})\\log(1-a^{(i)})\n",
    "$$\n",
    "### 1.2. Backward propagation\n",
    "---\n",
    "Gradients:\n",
    "$$ \\large \\frac{\\partial J}{\\partial w} = \\frac{1}{m}X(A-Y)^T$$\n",
    "$$ \\large \\frac{\\partial J}{\\partial b} = \\frac{1}{m} \\sum_{i=1}^m (a^{(i)}-y^{(i)})$$\n",
    "where:\n",
    "- $m$ is the number of examples in the dataset;\n",
    "- $a^{(i)}$ is the $i_{th}$ component of vector $A$.\n",
    "### 1.3. Optimization\n",
    "---\n",
    "The optimization functions is:\n",
    "$$ \\large\n",
    "\\theta = \\theta - \\alpha d\\theta\n",
    "$$\n",
    "where $\\alpha$ is the *learning rage*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Perceptron(object):\n",
    "    def __init__(self, dim=2, alpha=0.01, iters=16, seed=1):\n",
    "        super(Perceptron, self).__init__()\n",
    "        self._iters = iters\n",
    "        self._alpha = alpha\n",
    "        np.random.seed(seed)\n",
    "        self._w = np.random.randn(dim, 1)\n",
    "        self._b = np.random.random()\n",
    "        self._J = 0.0\n",
    "    \n",
    "    @property\n",
    "    def J(self):\n",
    "        return self._J\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        w, b = self._w, self._b\n",
    "        alpha = self._alpha\n",
    "        m = y.shape[1]\n",
    "        for i in range(self._iters):\n",
    "            Z = np.dot(w.T, X) + b\n",
    "            A = 1/(1 + np.exp(-Z))\n",
    "            dw = 1/m*np.dot(X, (A - y).T)\n",
    "            db = 1/m*np.sum(A - y, axis=1)\n",
    "            w = w - alpha*dw\n",
    "            b = b - alpha*db\n",
    "        self._w, self._b = w, b\n",
    "        self._J = -1/m*np.sum(y*np.log(A) + (1 - y)*np.log(1 - A), axis=1)\n",
    "        self._J = np.squeeze(self._J)\n",
    "    \n",
    "    def pred(self, x, beta=0.5):\n",
    "        w, b = self._w, self._b\n",
    "        z = np.dot(w.T, x) + b\n",
    "        a = 1/(1 + np.exp(-z))\n",
    "        return a > beta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Classification\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1.81 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Perceptron model\n",
    "p = Perceptron(iters=20000)\n",
    "\n",
    "# Training and Prediction\n",
    "p.fit(x1, y1)\n",
    "y1_hat = p.pred(X)\n",
    "\n",
    "p.fit(x2, y2)\n",
    "y2_hat = p.pred(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![nn perceptron](output/NN_perceptron.gif \"Neural Network Perceptron\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
